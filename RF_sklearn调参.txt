ensemble
随机森林实际上是一种特殊的bagging方法，它将决策树用作bagging中的模型。
首先，用bootstrap方法生成m个训练集，
然后，对于每个训练集，构造一颗决策树，
在节点找特征进行分裂的时候，并不是对所有特征找到能使得指标（如信息增益）最大的，
而是在特征中随机抽取一部分特征，在抽到的特征中间找到最优解，应用于节点，进行分裂。
随机森林的方法由于有了bagging，也就是集成的思想在，实际上相当于对于样本和特征都进行了采样
（如果把训练数据看成矩阵，就像实际中常见的那样，那么就是一个行和列都进行采样的过程），所以可以避免过拟合。

prediction阶段的方法就是bagging的策略，分类投票，回归均值。



max_features:
          随机森林允许单个决策树使用特征的最大数量.
          e.g. Auto.None:简单选取所有特征，每棵树都可以利用他们。
              sqrt:此选项是每颗子树可以利用总特征数的平方根个，如果变量特征总数为100，每科子树只能取10个。“log2”是另一种相似类型的选项。
              0.2: 此选项允许每个随机森林的子树可以利用变量（特征）数的20%。
              
           max_features如何影响性能和速度：
           增加max_features一般能提高模型的性能，因为在每个节点上，我们有更多的选择可以考虑。
           然而，这未必完全是对的，因为它降低了单个树的多样性，而这正是随机森林独特的优点。
n_estimators:
          在利用最大投票数或平均值来预测之前，你想要建立子树的数量。
          较多的子树可以让模型有更好的性能，但同时让你的代码变慢。
          你应该选择尽可能高的值，只要你的处理器能够承受的住，因为这使你的预测更好更稳定。

min_sample_leaf:
          是决策树的末端节点。 较小的叶子使模型更容易捕捉训练数据中的噪声。
          一般来说，最小叶子节点数目设置为大于50。在不同的情况中，你应该尽量尝试多种叶子大小种类，以找到最优的那个。

n_jobs:
          这个参数告诉引擎有多少处理器是它可以使用。 “-1”意味着没有限制，而“1”值意味着它只能使用一个处理器。
random_state:
          此参数让结果容易复现。 一个确定的随机值将会产生相同的结果，在参数和训练数据不变的情况下。

oob_score:
          这是一个随机森林交叉验证方法。 它和留一验证方法非常相似
          这种方法只是简单的标记在每颗子树中用的观察数据。
          然后对每一个观察样本找出一个最大投票得分，是由那些没有使用该观察样本进行训练的子树投票得到。
          
